{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 Navigation Report\n",
    "\n",
    "### Training \n",
    "For the training session, we construct the **agent** with parameters\n",
    "and we run the *Deep-Q-Network* procedure **dqn** as follows:\n",
    "\n",
    "  agent = **Agent**(state_size=37, action_size=4, seed=1, n_fc1=64, n_fc2=64)       \n",
    "  scores, episodes = **dqn**(n_episodes = 1000, eps_start = epsilon_start)\n",
    "\n",
    "### Parameters\n",
    "We experience the following parameters:  **_n\\_fc1_** (No. of neurons in 1st Fully-Connected Layer), **_n\\_fc2_** (No. of neurons in 2nd Fully-Connected Layer), **_eps\\_start_** (Starting value of Epsilon, to perform Epsilon-Grredy Algorithm).\n",
    "For the training session, \n",
    " * _eps\\_start_ is played out as a value 0.99 with step 0.001, \n",
    " * _n\\_fc1_ is played out as a value 64,\n",
    " * _n\\_fc2_ is played out as a value 64.\n",
    " \n",
    "The other Hyperparameters used for solving the environment.\n",
    " * GAMMA -> 0.99 (To get the future expected rewards.)\n",
    " * TAU -> 1e-3 i.e 0.003 (To update the target model(store in my case) with local model(learn in my case).)\n",
    " * LR -> 5e-4 i.e 0.0005 (Learning Rate to update weights of the network during back-propagation.)\n",
    " * BATCH_SIZE -> 64 (Batch of 64 _sarsa_ tuples to feed to network for learning)\n",
    " * BUFFER_SIZE -> 100000 (Length of Replay Buffer)\n",
    " * UPDATE_EVERY -> 4 (To update target model after this many steps, with local model.)\n",
    "\n",
    "\n",
    "### Model Q-Network\n",
    "\n",
    "Both Q-Networks (learn and store) are implemented by the class\n",
    "**QNetwork**. This class implements the simple\n",
    "neural network with 3 fully-connected layers and 2 \n",
    "rectified nonlinear layers. This **QNetwork** is realized in the framework \n",
    "of package **PyTorch**. The number of neurons of the fully-connected layers are \n",
    "as follows:\n",
    "\n",
    " * Layer Fully-Connected Layer 1,  number of neurons: _state_size_ x _n\\_fc1_ , \n",
    " * Layer Fully-Connected Layer 2,  number of neurons: _n\\_fc1_ x _n\\_fc2_ ,\n",
    " * Layer Fully-Connected Layer 3,  number of neurons: _n\\_fc2_ x _action_size_ ,\n",
    " \n",
    "where _state_size_ = 37, _action_size_ = 8, _n\\_fc1_ and _n\\_fc2_\n",
    "are the input params as given above.\n",
    "\n",
    "### Learning Algrorithm\n",
    "\n",
    "#### Deep-Q-Network algorithm\n",
    "\n",
    "The _Deep-Q-Network_ procedure **dqn** performs the **double loop**. \n",
    "External loop (by _episodes_) is executed till the number of episodes reached the maximum **number \n",
    "of episodes** = _1000_ or the _completion criteria_ is executed.\n",
    "For the completion criteria, we check  \n",
    "\n",
    "  _np.mean(scores_window) >=13_, (scores_window -> queue of 100 scores stored after each episode.)\n",
    "  \n",
    "In the internal loop,  **dqn** gets the current _action_ from the **agent**.\n",
    "By this _action_ **dqn** gets _state_ and _reward_ from Unity environment.\n",
    "Then, the **agent** accept params _state,action,reward,next_state, done_\n",
    "to the next training step. The variable _score_ accumulates obtained rewards.\n",
    "\n",
    "#### Mechanism of Agent\n",
    "* Two Q-Networks (learn and store) using the simple neural network as Model Q-Network shown above.\n",
    "```bash\n",
    "self.qnet_learn = QNetwork(state_size, action_size, seed, n_fc1, n_fc2).to(device)\n",
    "self.qnet_store = QNetwork(state_size, action_size, seed, n_fc1, n_fc2).to(device)\n",
    "```\n",
    "* Replay memory (using the class Experience)\n",
    "```bash\n",
    "self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "e = self.experience(state, action, reward, next_state, done)\n",
    "self.memory.append(e)\n",
    "```\n",
    "* Epsilon-greedy mechanism to get random actions sometimes.\n",
    "```bash\n",
    "if random.random() > eps:\n",
    "    return np.argmax(action_values.cpu().data.numpy())\n",
    "else:\n",
    "    return random.choice(np.arange(self.action_size))\n",
    "```\n",
    "* Epsilon becomes smaller after each episode.\n",
    "```bash\n",
    "eps = max(eps_end,eps_decay*eps)\n",
    "```\n",
    "* Q-learning, i.e., using the max value for all possible actions\n",
    "* Computing the loss function by MSE loss\n",
    "```bash\n",
    "loss = F.mse_loss(Q_expected, Q_targets)\n",
    "```\n",
    "* Minimize the loss by gradient descend mechanism using the ADAM optimizer.\n",
    "\n",
    "### Result Graph\n",
    "```bash\n",
    "agent = Agent(state_size=37, action_size=4, seed=1)\n",
    "scores, ep = dqn(n_episodes=1000, eps_start=.99, eps_end=0.01, eps_decay = .996)\n",
    "```\n",
    "Episode: 587, elapsed: 0:07:44.751907, Avg.Score: 13.02,  score 15.0, How many scores >= 13: 60, eps.: 0.09<br>\n",
    " terminating at episode : 587 ave reward reached +13 over 100 episodes\n",
    "![Result Graph](plot.png)\n",
    "\n",
    "Using The Above Given Parameters, this environment was solved within 600 episodes i.e 587 episodes.<br>\n",
    "This can be improved by applying below given Future Ideas.\n",
    "### Future Ideas\n",
    "1. Improvement can be done by adding one or more layers to the network.\n",
    "2. Changing the number of neurons in each layer can also improve the learning curve between states and actions.\n",
    "3. Updating epsilon with different decay or starting with different epsilon can be a improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
