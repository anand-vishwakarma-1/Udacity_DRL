{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1 Navigation Report\n",
    "\n",
    "### Training \n",
    "For the training session, we construct the **agent** with parameters\n",
    "and we run the *Deep-Q-Network* procedure **dqn** as follows:\n",
    "\n",
    "  agent = **Agent**(state_size=37, action_size=4, seed=1, n_fc1=64, n_fc2=64)       \n",
    "  scores, episodes = **dqn**(n_episodes = 1000, eps_start = epsilon_start)\n",
    "\n",
    "### Parameters\n",
    "We experience the following parameters:  _n_fc1_, _n_fc2_,  _eps_start_.\n",
    "For the training session, \n",
    " * _eps_start_ is played out as a value 0.9 with step 0.001, \n",
    " * _n_fc1_ is played out as a value 64,\n",
    " * _n_fc2_ is played out as a value 64.\n",
    " \n",
    "\n",
    "### Deep-Q-Network algorithm\n",
    "\n",
    "The _Deep-Q-Network_ procedure **dqn** performs the **double loop**. \n",
    "External loop (by _episodes_) is executed till the number of episodes reached the maximum number \n",
    "of episodes _n_episodes = 2000_ or the _completion criteria_ is executed.\n",
    "The environment _env_  is reset with the paarmeter _train_mode_=_True_.\n",
    "For the completion criteria, we check  \n",
    "\n",
    "  _np.mean(scores_window) >=13_,\n",
    "  \n",
    "In the internal loop,  **dqn** gets the current _action_ from the **agent**.\n",
    "By this _action_ **dqn** gets _state_ and _reward_ from Unity environment.\n",
    "Then, the **agent** accept params _state,action,reward,next_state, done_\n",
    "to the next training step. The variable _score_ accumulates obtained rewards.\n",
    "\n",
    "\n",
    "### Model Q-Network\n",
    "\n",
    "Both Q-Networks (learn and store) are implemented by the class\n",
    "**QNetwork**. This class implements the simple\n",
    "neural network with 3 fully-connected layers and 2 \n",
    "rectified nonlinear layers. This **QNetwork** is realized in the framework \n",
    "of package **PyTorch**. The number of neurons of the fully-connected layers are \n",
    "as follows:\n",
    "\n",
    " * Layer fc1,  number of neurons: _state_size_ x _n_fc1_, \n",
    " * Layer fc2,  number of neurons: _n_fc1_ x _n_fc2_,\n",
    " * Layer fc3,  number of neurons: _n_fc2_ x _action_size_,\n",
    " \n",
    "where _state_size_ = 37, _action_size_ = 8, _n_fc1_ and _n_fc2_\n",
    "are the input params.\n",
    "\n",
    "### Learning Algrorithm\n",
    "1. Two Q-Networks (learn and store) using the simple neural network as Model Q-Network shown above.\n",
    "2. Replay memory (using the class Experience)\\\n",
    "3. Epsilon-greedy mechanism to get random actions sometimes.\n",
    "4. Epsilon becomes smaller after each episode.\n",
    "5. Minimize the loss by gradient descend mechanism using the ADAM optimizer.\n",
    "\n",
    "### Result Graph\n",
    "![Result Graph](plot.png)\n",
    "\n",
    "### Future Ideas\n",
    "1. Improve can be done by adding one or more layers to the network.\n",
    "2. Changing the number of neurons in each layer can also improve the learning curve between states and actions.\n",
    "3. Updating epsilon with different decay or starting with different epsilon can be a improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
