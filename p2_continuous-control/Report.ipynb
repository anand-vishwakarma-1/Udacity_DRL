{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Continous control\n",
    "\n",
    "### Training \n",
    "For the training session, we construct the **agent** with above parameters\n",
    "and we run the *Deep-Q-Network* procedure **ddpg** as follows:\n",
    "\n",
    "  agent = **Agent**(state_size=state_size, action_size=action_size, random_seed=8)<br>\n",
    "  scores = **ddpg**()\n",
    "\n",
    "### Parameters\n",
    "We experience the following parameters:  **_n\\_fc1_** (No. of neurons in 1st Fully-Connected Layer), **_n\\_fc2_** (No. of neurons in 2nd Fully-Connected Layer),\n",
    " * _n\\_fc1_ is played out as a value 64,\n",
    " * _n\\_fc2_ is played out as a value 64.\n",
    " \n",
    "The other Hyperparameters used for solving the environment.\n",
    " * GAMMA -> 0.99 (To get the future expected rewards.)\n",
    " * TAU -> 1e-3 i.e 0.003 (To update the target model with local model.)\n",
    " * BATCH_SIZE -> 64 (Batch of 64 _sarsa_ tuples to feed to network for learning)\n",
    " * BUFFER_SIZE -> 100000 (Length of Replay Buffer)\n",
    " * LR_ACTOR = 1e-3 (Learning Rate to update weights of the Actor network during back-propagation.)\n",
    " * LR_CRITIC = 1e-3 (Learning Rate to update weights of the Critic network during back-propagation.)\n",
    " * WEIGHT_DECAY = 0 (Weight Decay for optimizer)\n",
    " * EPSILON = 1.0 (Starting value of Epsilon, to perform Epsilon-Grredy Algorithm)\n",
    " * EPSILON_DECAY = 1e-6 (Decay for Epsilon value)\n",
    " * LEARNING_PERIOD = 20 (learning frequency)\n",
    " * UPDATE_FACTOR   = 10 (how much to learn)\n",
    "\n",
    "\n",
    "### Model Networks (Policy and Value)\n",
    "\n",
    "Both Policy and Value networks implements the simple\n",
    "neural network with 3 fully-connected layers and 2 \n",
    "rectified nonlinear layers. This is realized in the framework \n",
    "of package **PyTorch**. The number of neurons of the fully-connected layers are \n",
    "as follows:\n",
    "\n",
    "For policy:\n",
    " * Fully-Connected Layer 1,  number of neurons: _state_size_ x _n\\_fc1_ , \n",
    " * Fully-Connected Layer 2,  number of neurons: _n\\_fc1_ x _n\\_fc2_ ,\n",
    " * Fully-Connected Layer 3,  number of neurons: _n\\_fc2_ x _action_size_ ,\n",
    " \n",
    "For Value:\n",
    " * Fully-Connected Layer 1,  number of neurons: _state_size_ x _n\\_fc1_ , \n",
    " * Fully-Connected Layer 2,  number of neurons: _(n\\_fc1+action\\_size)_ x _n\\_fc2_ ,\n",
    " * Fully-Connected Layer 3,  number of neurons: _n\\_fc2_ x 1 ,\n",
    "\n",
    "### Learning Algrorithm\n",
    "\n",
    "#### Deep Deterministic Policy Gradient algorithm\n",
    "\n",
    "DDPG is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy. This dual mechanism is the actor-critic method. The DDPG algorithm uses two additional mechanisms: Replay Buffer and Soft Updates.\n",
    "\n",
    "####     Actor-Critic dual mechanism\n",
    "\n",
    "For each timestep _t,_ we do the following operations:\n",
    "\n",
    "Let __*S&nbsp;*__ be the current state. It is the  input for the  _Actor NN_.  The output is the action-value \n",
    "\n",
    "![](images/policy_pi.png)\n",
    "\n",
    "where \\pi is the policy function,  i.e., the distribution of the actions. The _Critic NN_  gets the state __*S&nbsp;*__ as input and outputs      \n",
    "the state-value function __*v(S,w)*__ , that is the _expected total reward_ for the agent starting from state __*S&nbsp;*__. Here, _\\theta_ is    \n",
    "the vector parameter of the _Actor NN_, _w&nbsp;_ - the vector parameter of the _Critic NN_. The task is to train both networks, i.e.,   \n",
    "to find the optimal values for _\\theta_ and _w&nbsp;_.  By policy _\\pi_ we get the action _A&nbsp;_,  from the environment we get reward _R&nbsp;_   \n",
    "and the next state __*S'&nbsp;*__. Then we get _TD-estimate_: \n",
    " \n",
    "![](images/TD_estimate.png)\n",
    "\t\t \n",
    "Next, we use the _Critic_ to calculate the _advantage function_ _A(s, a)_:\n",
    "\n",
    "![](images/calc_advantage.png)\n",
    "\t\t\t\t \n",
    "Here, _\\gamma_ is the _discount factor_. The parameter _\\theta_ is updated by gradient ascent as follows:\n",
    "\n",
    "![](images/update_theta.png)\n",
    "\n",
    "The parameter _w&nbsp;_ is updated as follows:\n",
    "\n",
    "![](images/update_w.png)\n",
    "\t\t\n",
    "Here, \\alpha (resp. \\beta) is the learning rate for the _Actor NN_ (resp. _Critic NN_).  Before we return to the next timestep we update the state _S&nbsp;_ and the operator _I&nbsp;_ by _discount factor_ \\gamma:\n",
    "\n",
    "![](images/next_state.png)\n",
    "\n",
    "At the start of the algorithm the operator _I_ should be initialized to the identity opeartor. \n",
    "#### Goal of Agent\n",
    "The environment for this project involves controlling a double-jointed arm, to reach target locations. A reward of +0.1 is provided for each step that the agent's hand is in the goal location. Thus, the goal of this agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "\n",
    "#### Mechanism of Agent\n",
    "* 4 Networks Q-Networks, Actor and Critic each having 2 networks as follows.\n",
    "    * Actor\n",
    "```bash\n",
    "self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "```\n",
    "    * Critic\n",
    "```bash\n",
    "self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "```\n",
    "* Replay memory (using the class Experience)\n",
    "```bash\n",
    "self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "e = self.experience(state, action, reward, next_state, done)\n",
    "self.memory.append(e)\n",
    "```\n",
    "* Update Critic Network\n",
    "```bash\n",
    "# Get predicted next-state actions and Q values from target models\n",
    "actions_next = self.actor_target(next_states)\n",
    "Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "# Compute Q targets for current states (y_i)\n",
    "Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "# Compute critic loss\n",
    "Q_expected = self.critic_local(states, actions)\n",
    "critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "# Minimize the loss\n",
    "self.critic_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "self.critic_optimizer.step()\n",
    "```\n",
    "* Update Actor\n",
    "```bash\n",
    "# Compute actor loss\n",
    "actions_pred = self.actor_local(states)\n",
    "actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "# Minimize the loss\n",
    "self.actor_optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "self.actor_optimizer.step()\n",
    "```\n",
    "\n",
    "### Result Graph\n",
    "```bash\n",
    "agent = Agent(state_size=state_size, action_size=action_size, random_seed=8)\n",
    "scores = ddpg()\n",
    "```\n",
    "\\*** Episode 261\tAverage Score: 30.11, Time: 02:19:15 *** <br>\n",
    "Environment solved !\n",
    "![Result Graph](plot.png)\n",
    "\n",
    "Using The Above Given Parameters, this environment was solved within 270 episodes i.e 261 episodes.<br>\n",
    "This can be improved by applying below given Future Ideas.\n",
    "### Future Ideas\n",
    "1. Improvement can be done by adding one or more layers to the network.\n",
    "2. Changing the number of neurons in each layer can also improve the learning curve between states and actions.\n",
    "3. Updating epsilon with different decay or starting with different epsilon can be a improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
