{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2 Collaboration and Competition\n",
    "\n",
    "### Training \n",
    "```bash\n",
    "maddpg = AgentHandler()\n",
    "dir_chkpoints = ''\n",
    "scores_total, scores_global = train(maddpg, env, dir_chkpoints, n_episodes=10000)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "There are total 8 networks, 4 for each agent, in which each Actor and Critic have local and target networks.\n",
    "For the training session for Actor Networks, \n",
    " * _n\\_fc1_ is played out as a value 32,\n",
    " * _n\\_fc2_ is played out as a value 32.\n",
    "\n",
    "For the training session for Critic Networks, \n",
    " * _n\\_fcs1_ is played out as a value 64,\n",
    " * _n\\_fc2_ is played out as a value 64.\n",
    " \n",
    "The other Hyperparameters used for solving the environment.\n",
    " * GAMMA -> 0.99 (To get the future expected rewards.)\n",
    " * TAU -> 5e-2 i.e 0.05 (To update the target model with local model.)\n",
    " * BATCH_SIZE -> 64 (Batch of 64 _sarsa_ tuples to feed to network for learning)\n",
    " * BUFFER_SIZE -> 1e6 i.e 1000000 (Length of Replay Buffer)\n",
    " * LR_ACTOR = 5e-4 (Learning Rate to update weights of the Actor network during back-propagation.)\n",
    " * LR_CRITIC = 5e-4 (Learning Rate to update weights of the Critic network during back-propagation.)\n",
    " * WEIGHT_DECAY = 0 (Weight Decay for optimizer)\n",
    " * NOISE_AMPLIFICATION = 1  (exploration noise amplification)\n",
    " * NOISE_AMPLIFICATION_DECAY = 1  (noise amplification decay)\n",
    " * LEARNING_PERIOD = 2  (weight update frequency)\n",
    "\n",
    "\n",
    "### Model Networks (Policy and Value)\n",
    "\n",
    "There are total 8 networks, 4 for each agent, in which each Actor and Critic have local and target networks.\n",
    "\n",
    "For Actor:<br>\n",
    "Fully-Connected Layer 1, number of neurons: _state\\_size_ x _n\\_fc1_ ,<br>\n",
    "Fully-Connected Layer 2, number of neurons: _n\\_fc1_ x _n\\_fc2_ ,<br>\n",
    "Fully-Connected Layer 3, number of neurons: _n\\_fc2_ x _action_size_ ,<br>\n",
    " \n",
    "For Critic:<br>\n",
    "Fully-Connected Layer 1, number of neurons: _(state\\_size + action\\_size) x n_agents_ x _n_fcs1_ ,<br>\n",
    "Fully-Connected Layer 2, number of neurons: _n\\_fcs1_ x _n\\_fc2_ ,<br>\n",
    "Fully-Connected Layer 3, number of neurons: _n\\_fc2_ x 1 ,<br>\n",
    "\n",
    "### Learning Algrorithm\n",
    "\n",
    "#### Deep Deterministic Policy Gradient algorithm\n",
    "\n",
    "DDPG is an algorithm which concurrently learns a Q-function and a policy. It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy. This dual mechanism is the actor-critic method. The DDPG algorithm uses two additional mechanisms: Replay Buffer and Soft Updates.\n",
    "\n",
    "#### Multi Agent Deep Deterministic Policy Gradient algorithm\n",
    "\n",
    "In this project, we use the **DDPG** algorithm (Deep Deterministic Policy Gradient) and the **MADDPG** algorithm,     \n",
    "a wrapper for DDPG. MADDPG stands for **Multi-Agent DDPG**. DDPG is an algorithm which concurrently   learns    \n",
    "a Q-function and a policy.  It uses off-policy data and the Bellman equation to learn the Q-function, \n",
    "and uses    \n",
    "the Q-function to learn the policy. This dual mechanism is the  actor-critic method. The DDPG algorithm uses   \n",
    "two additional mechanisms: _Replay Buffer_ and _Soft Updates_.  \n",
    "\n",
    "In MADDPG, we train two separate agents, and the agents need to **collaborate** (like don’t let the   ball hit the ground)   \n",
    "and **compete** (like gather as many points as possible). Just doing a simple extension of single \n",
    "agent RL    \n",
    "by independently training the two agents does not work very well because the agents are independently updating    \n",
    "their policies as learning progresses. And this causes the   environment to appear non-stationary from the viewpoint   \n",
    "of any one agent. \n",
    "\n",
    "In MADDPG, _each agent’s critic is trained using the observations and actions_ from **both agents** , whereas   \n",
    "each _agent’s actor is trained using just_ its **own observations**.  \n",
    "\n",
    "In the finction _step()_ of the _class madppg_\\__agent_, we collect all current info\n",
    " for **both agents**  into  the **common** variable    \n",
    "_memory_ of the type  _ReplayBuffer_.  Then we get the random _sample_ from _memory_  into the variable _experiance_.   \n",
    "This _experiance_   together with the current number of agent (0 or 1) go to the function _learn()_.   We get the corresponding    \n",
    "agent (of type _ddpg_\\__agent_):\n",
    "\n",
    "      agent = self.agents[agent_number]\n",
    "\n",
    "and _experiance_ is transferred to function _learn()_  of the _class ddpg_\\__agent_.  There, the actor and the critic \n",
    "are handled by different ways.  \n",
    "\n",
    "####     Actor-Critic dual mechanism\n",
    "\n",
    "For each timestep _t,_ we do the following operations:\n",
    "\n",
    "Let __*S&nbsp;*__ be the current state. It is the  input for the  _Actor NN_.  The output is the action-value \n",
    "\n",
    "![](images/policy_pi.png)\n",
    "\n",
    "where \\pi is the policy function,  i.e., the distribution of the actions. The _Critic NN_  gets the state __*S&nbsp;*__ as input and outputs      \n",
    "the state-value function __*v(S,w)*__ , that is the _expected total reward_ for the agent starting from state __*S&nbsp;*__. Here, _\\theta_ is    \n",
    "the vector parameter of the _Actor NN_, _w&nbsp;_ - the vector parameter of the _Critic NN_. The task is to train both networks, i.e.,   \n",
    "to find the optimal values for _\\theta_ and _w&nbsp;_.  By policy _\\pi_ we get the action _A&nbsp;_,  from the environment we get reward _R&nbsp;_   \n",
    "and the next state __*S'&nbsp;*__. Then we get _TD-estimate_: \n",
    " \n",
    "![](images/TD_estimate.png)\n",
    "\t\t \n",
    "Next, we use the _Critic_ to calculate the _advantage function_ _A(s, a)_:\n",
    "\n",
    "![](images/calc_advantage.png)\n",
    "\t\t\t\t \n",
    "Here, _\\gamma_ is the _discount factor_. The parameter _\\theta_ is updated by gradient ascent as follows:\n",
    "\n",
    "![](images/update_theta.png)\n",
    "\n",
    "The parameter _w&nbsp;_ is updated as follows:\n",
    "\n",
    "![](images/update_w.png)\n",
    "\t\t\n",
    "Here, \\alpha (resp. \\beta) is the learning rate for the _Actor NN_ (resp. _Critic NN_).  Before we return to the next timestep we update the state _S&nbsp;_ and the operator _I&nbsp;_ by _discount factor_ \\gamma:\n",
    "\n",
    "![](images/next_state.png)\n",
    "\n",
    "At the start of the algorithm the operator _I_ should be initialized to the identity opeartor. \n",
    "\n",
    "#### Mechanism of each Agent\n",
    "* 4 Networks Q-Networks, Actor and Critic each having 2 networks as follows.\n",
    "    * Actor\n",
    "```bash\n",
    "self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "```\n",
    "    * Critic\n",
    "```bash\n",
    "self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "```\n",
    "* Replay memory (using the class Experience)\n",
    "```bash\n",
    "self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "e = self.experience(state, action, reward, next_state, done)\n",
    "self.memory.append(e)\n",
    "```\n",
    "* Update Critic Network\n",
    "```bash\n",
    "# Get predicted next-state actions and Q values from target models\n",
    "actions_next = self.actor_target(next_states)\n",
    "Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "# Compute Q targets for current states (y_i)\n",
    "Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "# Compute critic loss\n",
    "Q_expected = self.critic_local(states, actions)\n",
    "critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "# Minimize the loss\n",
    "self.critic_optimizer.zero_grad()\n",
    "critic_loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)\n",
    "self.critic_optimizer.step()\n",
    "```\n",
    "* Update Actor\n",
    "```bash\n",
    "# Compute actor loss\n",
    "actions_pred = self.actor_local(states)\n",
    "actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "# Minimize the loss\n",
    "self.actor_optimizer.zero_grad()\n",
    "actor_loss.backward()\n",
    "self.actor_optimizer.step()\n",
    "```\n",
    "\n",
    "### Result Graph\n",
    "```bash\n",
    "maddpg = AgentHandler()\n",
    "dir_chkpoints = ''\n",
    "scores_total, scores_global = train(maddpg, env, dir_chkpoints, n_episodes=10000)\n",
    "```\n",
    "<p>\n",
    "*** Environment solved in 6424 episodes!\tAverage Score: 0.52 ***\n",
    "\n",
    "Episode: 6450, Score: 1.8450, \tAverage Score: 0.6923, Time: 00:31:38<br> \n",
    "\\*** Episode 6450\tAverage Score: 0.69, Time: 00:31:38 ***\n",
    " \n",
    "Episode: 6500, Score: 2.6000, \tAverage Score: 1.2494, Time: 00:37:05 <br>\n",
    "\\*** Episode 6500\tAverage Score: 1.25, Time: 00:37:05 ***\n",
    " \n",
    "Episode: 6550, Score: 2.6000, \tAverage Score: 1.6173, Time: 00:42:26 <br>\n",
    "\\*** Episode 6550\tAverage Score: 1.62, Time: 00:42:26 ***\n",
    "</p>\n",
    "\n",
    "![Result Graph](plot.png)\n",
    "\n",
    "Using The Above Given Parameters, this environment was solved within 270 episodes i.e 261 episodes.<br>\n",
    "This can be improved by applying below given Future Ideas.\n",
    "\n",
    "### Future Ideas\n",
    "1. Improvement can be done by adding one or more layers to the network.\n",
    "2. Changing the number of neurons in each layer can also improve the learning curve between states and actions.\n",
    "3. Updating epsilon with different decay or starting with different epsilon can be a improvement.\n",
    "4. NAF can be used as a alternative to DDPG as\n",
    " 1. NAF learns a smooth, stable policy, whereas DDPG learns an unstable policy\n",
    " 2. Therefore NAF is more suitable for domains where precision is required (robot arm manip e.g.)\n",
    " 3. NAF performs better than DDPG on 80% or so of tested tasks\n",
    "5. Also MAPPO i.e multiagent PPO can be used here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
